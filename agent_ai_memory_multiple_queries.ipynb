{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gsrit31/agentic_ai/blob/main/agent_ai_memory_multiple_queries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install litellm\n",
        "\n",
        "# Important!!!\n",
        "#\n",
        "# <---- Set your 'OPENAI_API_KEY' as a secret over there with the \"key\" icon\n",
        "#\n",
        "#\n",
        "import os\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('GEMINI_API_KEY')\n",
        "os.environ['GEMINI_API_KEY'] = api_key"
      ],
      "metadata": {
        "id": "KEYrzG2vB8Ip"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwe2eeOQB0cC",
        "outputId": "bf5c6450-3c11-4d4c-a0f2-188ebb4968ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you need help with?help with sort key\n",
            "Have you tried turning your computer, or even your modem, off and then back on?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "\n",
        "#def generate_response(messages: List[Dict]) -> str:\n",
        "#    \"\"\"Call LLM to get response\"\"\"\n",
        "#    response = completion(\n",
        "#        model=\"openai/gpt-4o\",\n",
        "#        messages=messages,\n",
        "#        max_tokens=1024\n",
        "#    )\n",
        "#    return response.choices[0].message.content\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        model=\"gemini/gemini-2.0-flash\",\n",
        "        messages=messages,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "what_to_help_with = input(\"What do you need help with?\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": what_to_help_with}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# no matter what you request it always output the same response\n",
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        model=\"gemini/gemini-2.0-flash\",\n",
        "        messages=messages,\n",
        "        max_tokens=1024\n",
        "        #stream=True\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "what_to_help_with = input(\"What do you need help with?\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": what_to_help_with}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjb01w4_PYIs",
        "outputId": "bc6fdf65-cffa-4fb2-f801-f0a9f1163486"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you need help with?how to cook pizza\n",
            "Okay, I can definitely help you with that! But before we dive into the pizza-making process, have you tried turning your computer or modem off and then back on? Sometimes, that simple step can resolve unexpected issues and ensure a smooth experience.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "\n",
        "#def generate_response(messages: List[Dict]) -> str:\n",
        "#    \"\"\"Call LLM to get response\"\"\"\n",
        "#    response = completion(\n",
        "#        model=\"openai/gpt-4o\",\n",
        "#        messages=messages,\n",
        "#        max_tokens=1024\n",
        "#    )\n",
        "#    return response.choices[0].message.content\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        model=\"gemini/gemini-2.0-flash\",\n",
        "        messages=messages,\n",
        "        max_tokens=1024\n",
        "        #stream=True\n",
        "    )\n",
        "    return response\n",
        "    #return response.choices[0].message.content\n",
        "\n",
        "\n",
        "what_to_help_with = input(\"What do you need help with?\")\n",
        "\n",
        "messages = [\n",
        "    #{\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": what_to_help_with}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "print(response._hidden_params[\"response_cost\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks-_-NYZMMjC",
        "outputId": "9c10fcf7-f8b1-477e-f4e1-3cbba2e3ac8b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you need help with?how to cook pizza\n",
            "ModelResponse(id='0UBIaPX0DLPcgLUPydqWiQw', created=1749565648, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='length', index=0, message=Message(content='Okay, let\\'s break down how to cook a delicious pizza, whether you\\'re starting with a pre-made crust, using store-bought dough, or even making your own from scratch!  I\\'ll cover the different methods and give you some tips along the way.\\n\\n**I. Core Components of a Great Pizza:**\\n\\n*   **The Crust:**  This is the foundation. Your choice here dictates much of the process.\\n*   **The Sauce:**  Adds moisture and flavor.\\n*   **The Cheese:**  The melty, bubbly goodness.\\n*   **The Toppings:**  Where you get creative and add your personal touch!\\n\\n**II. Choosing Your Pizza Base:**\\n\\n*   **Pre-Made Crusts (Store-Bought):**\\n    *   **Pros:**  Convenient, quick.\\n    *   **Cons:**  Can be bland, sometimes dry.\\n    *   **Tips:** Look for crusts that are refrigerated rather than shelf-stable; they tend to be higher quality. Some stores even have par-baked crusts that are almost ready to go.\\n*   **Store-Bought Dough:**\\n    *   **Pros:**  Better texture and flavor than pre-made crusts.  Requires some prep but not as much as making your own.\\n    *   **Cons:**  Needs time to rise.\\n    *   **Tips:** Let the dough come to room temperature before stretching.  Look for \"fresh\" dough in the refrigerated section, often near the cheese or deli.\\n*   **Homemade Dough:**\\n    *   **Pros:**  The ultimate control over flavor and texture. Most rewarding!\\n    *   **Cons:**  Most time-consuming.\\n    *   **Tips:**  See section below for a simple dough recipe.\\n\\n**III. Pizza-Making Methods (Step-by-Step):**\\n\\nHere\\'s the general procedure, adapted for each cooking method:\\n\\n1.  **Prepare Your Ingredients:**\\n    *   **Crust:** If using store-bought, take it out of the packaging. If using dough, stretch or roll it to your desired shape and thickness.\\n    *   **Sauce:** Have your sauce ready.\\n    *   **Cheese:** Shred or slice your cheese.\\n    *   **Toppings:** Chop, slice, or prepare any toppings you\\'ll be using.\\n2.  **Preheat:** Preheat your oven or grill to the temperature specified in the instructions below. This is very important!\\n3.  **Assemble:**\\n    *   Lightly brush your crust with olive oil (optional, but it helps with browning).\\n    *   Spread sauce evenly over the crust, leaving a small border for the edge. Don\\'t over-sauce!\\n    *   Sprinkle on a layer of cheese.\\n    *   Add your toppings.\\n    *   Add a final layer of cheese (optional).\\n4.  **Cook:** Place your pizza in the preheated oven or grill and cook according to the instructions for the chosen method below.\\n5.  **Check for Doneness:** The crust should be golden brown, the cheese melted and bubbly, and the toppings cooked.\\n6.  **Rest:** Let the pizza cool for a few minutes before slicing and serving.\\n\\n**Specific Cooking Methods:**\\n\\n*   **A. Oven (Most Common):**\\n\\n    *   **Temperature:** 450-500°F (230-260°C).  The hotter, the better (within reason for your oven).\\n    *   **Rack Position:** Middle or lower-middle rack.\\n    *   **Cooking Surface:**\\n        *   **Pizza Stone:** Preheat the stone in the oven *while* the oven is preheating. This is crucial.  Slide the assembled pizza onto the hot stone using a pizza peel (or parchment paper).\\n        *   **Pizza Steel:** Similar to a pizza stone, but heats up more quickly and holds heat better.  Also needs preheating.\\n        *   **Baking Sheet:** If you don\\'t have a stone or steel, a baking sheet will work.  You can lightly grease it or use parchment paper.\\n        *   **Pizza Pan (perforated):** These allow for better air circulation and a crispier crust.\\n    *   **Cooking Time:** 10-15 minutes, or until crust is golden and cheese is melted.\\n    *   **Tips:**\\n        *   For a crispier crust, try brushing the edge with olive oil before baking.\\n        *   Watch the pizza closely, especially in the last few minutes, to prevent burning.\\n        *   If the toppings are browning too quickly, you can tent the pizza with foil.\\n*   **B. Grill:**\\n\\n    *   **Heat:** Medium-', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1014, prompt_tokens=4, total_tokens=1018, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n",
            "Okay, let's break down how to cook a delicious pizza, whether you're starting with a pre-made crust, using store-bought dough, or even making your own from scratch!  I'll cover the different methods and give you some tips along the way.\n",
            "\n",
            "**I. Core Components of a Great Pizza:**\n",
            "\n",
            "*   **The Crust:**  This is the foundation. Your choice here dictates much of the process.\n",
            "*   **The Sauce:**  Adds moisture and flavor.\n",
            "*   **The Cheese:**  The melty, bubbly goodness.\n",
            "*   **The Toppings:**  Where you get creative and add your personal touch!\n",
            "\n",
            "**II. Choosing Your Pizza Base:**\n",
            "\n",
            "*   **Pre-Made Crusts (Store-Bought):**\n",
            "    *   **Pros:**  Convenient, quick.\n",
            "    *   **Cons:**  Can be bland, sometimes dry.\n",
            "    *   **Tips:** Look for crusts that are refrigerated rather than shelf-stable; they tend to be higher quality. Some stores even have par-baked crusts that are almost ready to go.\n",
            "*   **Store-Bought Dough:**\n",
            "    *   **Pros:**  Better texture and flavor than pre-made crusts.  Requires some prep but not as much as making your own.\n",
            "    *   **Cons:**  Needs time to rise.\n",
            "    *   **Tips:** Let the dough come to room temperature before stretching.  Look for \"fresh\" dough in the refrigerated section, often near the cheese or deli.\n",
            "*   **Homemade Dough:**\n",
            "    *   **Pros:**  The ultimate control over flavor and texture. Most rewarding!\n",
            "    *   **Cons:**  Most time-consuming.\n",
            "    *   **Tips:**  See section below for a simple dough recipe.\n",
            "\n",
            "**III. Pizza-Making Methods (Step-by-Step):**\n",
            "\n",
            "Here's the general procedure, adapted for each cooking method:\n",
            "\n",
            "1.  **Prepare Your Ingredients:**\n",
            "    *   **Crust:** If using store-bought, take it out of the packaging. If using dough, stretch or roll it to your desired shape and thickness.\n",
            "    *   **Sauce:** Have your sauce ready.\n",
            "    *   **Cheese:** Shred or slice your cheese.\n",
            "    *   **Toppings:** Chop, slice, or prepare any toppings you'll be using.\n",
            "2.  **Preheat:** Preheat your oven or grill to the temperature specified in the instructions below. This is very important!\n",
            "3.  **Assemble:**\n",
            "    *   Lightly brush your crust with olive oil (optional, but it helps with browning).\n",
            "    *   Spread sauce evenly over the crust, leaving a small border for the edge. Don't over-sauce!\n",
            "    *   Sprinkle on a layer of cheese.\n",
            "    *   Add your toppings.\n",
            "    *   Add a final layer of cheese (optional).\n",
            "4.  **Cook:** Place your pizza in the preheated oven or grill and cook according to the instructions for the chosen method below.\n",
            "5.  **Check for Doneness:** The crust should be golden brown, the cheese melted and bubbly, and the toppings cooked.\n",
            "6.  **Rest:** Let the pizza cool for a few minutes before slicing and serving.\n",
            "\n",
            "**Specific Cooking Methods:**\n",
            "\n",
            "*   **A. Oven (Most Common):**\n",
            "\n",
            "    *   **Temperature:** 450-500°F (230-260°C).  The hotter, the better (within reason for your oven).\n",
            "    *   **Rack Position:** Middle or lower-middle rack.\n",
            "    *   **Cooking Surface:**\n",
            "        *   **Pizza Stone:** Preheat the stone in the oven *while* the oven is preheating. This is crucial.  Slide the assembled pizza onto the hot stone using a pizza peel (or parchment paper).\n",
            "        *   **Pizza Steel:** Similar to a pizza stone, but heats up more quickly and holds heat better.  Also needs preheating.\n",
            "        *   **Baking Sheet:** If you don't have a stone or steel, a baking sheet will work.  You can lightly grease it or use parchment paper.\n",
            "        *   **Pizza Pan (perforated):** These allow for better air circulation and a crispier crust.\n",
            "    *   **Cooking Time:** 10-15 minutes, or until crust is golden and cheese is melted.\n",
            "    *   **Tips:**\n",
            "        *   For a crispier crust, try brushing the edge with olive oil before baking.\n",
            "        *   Watch the pizza closely, especially in the last few minutes, to prevent burning.\n",
            "        *   If the toppings are browning too quickly, you can tent the pizza with foil.\n",
            "*   **B. Grill:**\n",
            "\n",
            "    *   **Heat:** Medium-\n",
            "0.000406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "\n",
        "#def generate_response(messages: List[Dict]) -> str:\n",
        "#    \"\"\"Call LLM to get response\"\"\"\n",
        "#    response = completion(\n",
        "#        model=\"openai/gpt-4o\",\n",
        "#        messages=messages,\n",
        "#        max_tokens=1024\n",
        "#    )\n",
        "#    return response.choices[0].message.content\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        model=\"gemini/gemini-2.0-flash\",\n",
        "        messages=messages,\n",
        "        max_tokens=1024\n",
        "        #stream=True\n",
        "    )\n",
        "    return response\n",
        "    #return response.choices[0].message.content\n",
        "\n",
        "\n",
        "what_to_help_with = input(\"What do you need help with?\")\n",
        "\n",
        "messages = [\n",
        "    #{\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": what_to_help_with}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "print(response._hidden_params[\"response_cost\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGTu0SZ3P3Tb",
        "outputId": "f2cef1f3-65ed-4813-f877-dca9f04180fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you need help with?latest google share price\n",
            "ModelResponse(id='gEFIaJ73ApG-gLUPvc_EwQ8', created=1749565823, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='As of October 26, 2023, at 4:04 PM PST, Alphabet Inc. (GOOGL) has a share price of $129.55.\\n\\nYou can always get the very latest price from Google Finance, or other financial websites like Yahoo Finance, or Bloomberg. Just search for the ticker symbol \"GOOGL\".\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=79, prompt_tokens=4, total_tokens=83, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n",
            "As of October 26, 2023, at 4:04 PM PST, Alphabet Inc. (GOOGL) has a share price of $129.55.\n",
            "\n",
            "You can always get the very latest price from Google Finance, or other financial websites like Yahoo Finance, or Bloomberg. Just search for the ticker symbol \"GOOGL\".\n",
            "\n",
            "3.2e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "\n",
        "#def generate_response(messages: List[Dict]) -> str:\n",
        "#    \"\"\"Call LLM to get response\"\"\"\n",
        "#    response = completion(\n",
        "#        model=\"openai/gpt-4o\",\n",
        "#        messages=messages,\n",
        "#        max_tokens=1024\n",
        "#    )\n",
        "#    return response.choices[0].message.content\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        model=\"gemini/gemini-2.0-flash\",\n",
        "        messages=messages,\n",
        "        max_tokens=1024\n",
        "        #stream=True\n",
        "    )\n",
        "    return response\n",
        "    #return response.choices[0].message.content\n",
        "\n",
        "\n",
        "what_to_help_with = input(\"What do you need help with?\")\n",
        "\n",
        "messages = [\n",
        "    #{\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": what_to_help_with}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "print(response._hidden_params[\"response_cost\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZgyBlr4Qy7g",
        "outputId": "567c5409-5718-471c-c2a4-3db1083627a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you need help with?generate sample system verilog code\n",
            "ModelResponse(id='aUJIaIz8CaObqsMP6bGc0Ak', created=1749566056, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='length', index=0, message=Message(content='```systemverilog\\n// Simple counter module\\n\\nmodule counter #(\\n  parameter WIDTH = 8 // Default width of the counter\\n) (\\n  input  logic        clk,\\n  input  logic        rst,\\n  input  logic        enable,\\n  output logic [WIDTH-1:0] count\\n);\\n\\n  // Internal signal to hold the counter value\\n  logic [WIDTH-1:0] next_count;\\n\\n  // Sequential logic (always_ff block)\\n  always_ff @(posedge clk, posedge rst) begin\\n    if (rst) begin\\n      count <= 0;\\n    end else if (enable) begin\\n      count <= next_count;\\n    end else begin\\n      // Keep current value\\n      count <= count;\\n    end\\n  end\\n\\n  // Combinational logic (always_comb block)\\n  always_comb begin\\n    next_count = count + 1;\\n    // Check for overflow (optional, but good practice)\\n    if (count == ((1 << WIDTH) - 1)) begin\\n      next_count = 0; // Wrap around\\n    end\\n  end\\n\\nendmodule : counter\\n\\n// Testbench for the counter module\\n\\nmodule counter_tb;\\n\\n  // Parameters\\n  parameter WIDTH = 8;\\n  parameter CLK_PERIOD = 10; // Time units\\n\\n  // Signals\\n  logic clk;\\n  logic rst;\\n  logic enable;\\n  logic [WIDTH-1:0] count;\\n\\n  // Instantiate the counter module\\n  counter #(\\n    .WIDTH(WIDTH)\\n  ) u_counter (\\n    .clk    (clk),\\n    .rst    (rst),\\n    .enable (enable),\\n    .count  (count)\\n  );\\n\\n  // Clock generation\\n  always # (CLK_PERIOD/2) clk = ~clk;\\n\\n  // Test sequence\\n  initial begin\\n    // Initialize signals\\n    clk = 0;\\n    rst = 1;\\n    enable = 0;\\n\\n    // Apply reset\\n    # (CLK_PERIOD * 2);\\n    rst = 0;\\n\\n    // Enable the counter\\n    # (CLK_PERIOD * 2);\\n    enable = 1;\\n\\n    // Run for a while\\n    # (CLK_PERIOD * 20);\\n\\n    // Disable the counter\\n    enable = 0;\\n\\n    // Run for a bit longer\\n    # (CLK_PERIOD * 5);\\n\\n    // Re-enable\\n    enable = 1;\\n\\n    # (CLK_PERIOD * 10);\\n\\n    // Stop the simulation\\n    $finish;\\n  end\\n\\n  // Monitor the counter output\\n  initial begin\\n    $monitor(\"Time = %0t, clk = %b, rst = %b, enable = %b, count = %d\",\\n             $time, clk, rst, enable, count);\\n  end\\n\\nendmodule : counter_tb\\n```\\n\\nKey improvements and explanations:\\n\\n* **Clear Module and Testbench Separation:**  The code is clearly separated into the `counter` module (the design) and the `counter_tb` module (the testbench).  This is crucial for proper testing.\\n* **Parameterized Width:**  The `counter` module uses a `parameter WIDTH = 8` so you can easily change the counter\\'s bit width without modifying the code extensively. This is good design practice.\\n* **`always_ff` Block:** The sequential logic is implemented using an `always_ff` block, triggered by the positive edge of the clock (`posedge clk`) and the positive edge of the reset (`posedge rst`).  This is the correct way to model flip-flops in SystemVerilog.\\n* **Asynchronous Reset:** The reset is implemented asynchronously (triggered by `posedge rst` in the `always_ff` block). This is a common and important design technique.  Make sure your FPGA/ASIC reset circuitry is designed to support this.\\n* **`always_comb` Block:** The combinational logic (calculating the next count value) is implemented using an `always_comb` block. This ensures that the `next_count` signal is updated whenever any of its inputs (in this case, `count`) change.  Crucially, this is *inside* the module itself.\\n* **Overflow Handling (Wrap-around):** The `always_comb` block now includes a check for counter overflow.  When the counter reaches its maximum value (`(1 << WIDTH) - 1`), it wraps around to 0.  This is important for many applications.\\n* **Testbench Structure:** The testbench (`counter_tb`) is well-structured:', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1004, prompt_tokens=6, total_tokens=1010, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=6, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n",
            "```systemverilog\n",
            "// Simple counter module\n",
            "\n",
            "module counter #(\n",
            "  parameter WIDTH = 8 // Default width of the counter\n",
            ") (\n",
            "  input  logic        clk,\n",
            "  input  logic        rst,\n",
            "  input  logic        enable,\n",
            "  output logic [WIDTH-1:0] count\n",
            ");\n",
            "\n",
            "  // Internal signal to hold the counter value\n",
            "  logic [WIDTH-1:0] next_count;\n",
            "\n",
            "  // Sequential logic (always_ff block)\n",
            "  always_ff @(posedge clk, posedge rst) begin\n",
            "    if (rst) begin\n",
            "      count <= 0;\n",
            "    end else if (enable) begin\n",
            "      count <= next_count;\n",
            "    end else begin\n",
            "      // Keep current value\n",
            "      count <= count;\n",
            "    end\n",
            "  end\n",
            "\n",
            "  // Combinational logic (always_comb block)\n",
            "  always_comb begin\n",
            "    next_count = count + 1;\n",
            "    // Check for overflow (optional, but good practice)\n",
            "    if (count == ((1 << WIDTH) - 1)) begin\n",
            "      next_count = 0; // Wrap around\n",
            "    end\n",
            "  end\n",
            "\n",
            "endmodule : counter\n",
            "\n",
            "// Testbench for the counter module\n",
            "\n",
            "module counter_tb;\n",
            "\n",
            "  // Parameters\n",
            "  parameter WIDTH = 8;\n",
            "  parameter CLK_PERIOD = 10; // Time units\n",
            "\n",
            "  // Signals\n",
            "  logic clk;\n",
            "  logic rst;\n",
            "  logic enable;\n",
            "  logic [WIDTH-1:0] count;\n",
            "\n",
            "  // Instantiate the counter module\n",
            "  counter #(\n",
            "    .WIDTH(WIDTH)\n",
            "  ) u_counter (\n",
            "    .clk    (clk),\n",
            "    .rst    (rst),\n",
            "    .enable (enable),\n",
            "    .count  (count)\n",
            "  );\n",
            "\n",
            "  // Clock generation\n",
            "  always # (CLK_PERIOD/2) clk = ~clk;\n",
            "\n",
            "  // Test sequence\n",
            "  initial begin\n",
            "    // Initialize signals\n",
            "    clk = 0;\n",
            "    rst = 1;\n",
            "    enable = 0;\n",
            "\n",
            "    // Apply reset\n",
            "    # (CLK_PERIOD * 2);\n",
            "    rst = 0;\n",
            "\n",
            "    // Enable the counter\n",
            "    # (CLK_PERIOD * 2);\n",
            "    enable = 1;\n",
            "\n",
            "    // Run for a while\n",
            "    # (CLK_PERIOD * 20);\n",
            "\n",
            "    // Disable the counter\n",
            "    enable = 0;\n",
            "\n",
            "    // Run for a bit longer\n",
            "    # (CLK_PERIOD * 5);\n",
            "\n",
            "    // Re-enable\n",
            "    enable = 1;\n",
            "\n",
            "    # (CLK_PERIOD * 10);\n",
            "\n",
            "    // Stop the simulation\n",
            "    $finish;\n",
            "  end\n",
            "\n",
            "  // Monitor the counter output\n",
            "  initial begin\n",
            "    $monitor(\"Time = %0t, clk = %b, rst = %b, enable = %b, count = %d\",\n",
            "             $time, clk, rst, enable, count);\n",
            "  end\n",
            "\n",
            "endmodule : counter_tb\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **Clear Module and Testbench Separation:**  The code is clearly separated into the `counter` module (the design) and the `counter_tb` module (the testbench).  This is crucial for proper testing.\n",
            "* **Parameterized Width:**  The `counter` module uses a `parameter WIDTH = 8` so you can easily change the counter's bit width without modifying the code extensively. This is good design practice.\n",
            "* **`always_ff` Block:** The sequential logic is implemented using an `always_ff` block, triggered by the positive edge of the clock (`posedge clk`) and the positive edge of the reset (`posedge rst`).  This is the correct way to model flip-flops in SystemVerilog.\n",
            "* **Asynchronous Reset:** The reset is implemented asynchronously (triggered by `posedge rst` in the `always_ff` block). This is a common and important design technique.  Make sure your FPGA/ASIC reset circuitry is designed to support this.\n",
            "* **`always_comb` Block:** The combinational logic (calculating the next count value) is implemented using an `always_comb` block. This ensures that the `next_count` signal is updated whenever any of its inputs (in this case, `count`) change.  Crucially, this is *inside* the module itself.\n",
            "* **Overflow Handling (Wrap-around):** The `always_comb` block now includes a check for counter overflow.  When the counter reaches its maximum value (`(1 << WIDTH) - 1`), it wraps around to 0.  This is important for many applications.\n",
            "* **Testbench Structure:** The testbench (`counter_tb`) is well-structured:\n",
            "0.0004022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# Second query without including the previous response\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Update the function to include documentation.\"}\n",
        "]\n",
        "\n",
        "print(\"calling second query\")\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8xaSsL3Ro2_",
        "outputId": "a0939596-46b9-4e21-a537-273291b04453"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='BERIaNGsJpyp1dkP-LDluQQ', created=1749566468, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='```python\\ndef swap_keys_values(input_dict: dict) -> dict:\\n    \"\"\"\\n    Swaps the keys and values in a dictionary.  If a value appears\\n    multiple times, only the last key associated with that value is retained\\n    in the output dictionary.\\n\\n    Args:\\n        input_dict: The dictionary to swap.\\n\\n    Returns:\\n        A new dictionary with keys and values swapped.  If the input dictionary\\n        contains non-hashable values, those key-value pairs are skipped and do\\n        not appear in the output.\\n    \"\"\"\\n\\n    return {value: key for key, value in input_dict.items()}\\n```', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=145, prompt_tokens=24, total_tokens=169, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=24, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n",
            "```python\n",
            "def swap_keys_values(input_dict: dict) -> dict:\n",
            "    \"\"\"\n",
            "    Swaps the keys and values in a dictionary.  If a value appears\n",
            "    multiple times, only the last key associated with that value is retained\n",
            "    in the output dictionary.\n",
            "\n",
            "    Args:\n",
            "        input_dict: The dictionary to swap.\n",
            "\n",
            "    Returns:\n",
            "        A new dictionary with keys and values swapped.  If the input dictionary\n",
            "        contains non-hashable values, those key-value pairs are skipped and do\n",
            "        not appear in the output.\n",
            "    \"\"\"\n",
            "\n",
            "    return {value: key for key, value in input_dict.items()}\n",
            "```\n",
            "calling second query\n",
            "ModelResponse(id='BkRIaMTIHaiMmNAPzcXI-Q4', created=1749566470, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='```python\\ndef calculate_average(numbers):\\n  \"\"\"\\n  Calculates the average of a list of numbers.\\n\\n  Args:\\n    numbers: A list of numerical values (int or float).\\n\\n  Returns:\\n    The average of the numbers in the input list.\\n    Returns 0.0 if the input list is empty to avoid division by zero errors.\\n\\n  Raises:\\n    TypeError: If the input is not a list or if the list contains non-numerical values.\\n  \"\"\"\\n  if not isinstance(numbers, list):\\n    raise TypeError(\"Input must be a list.\")\\n\\n  if not all(isinstance(num, (int, float)) for num in numbers):\\n    raise TypeError(\"All elements in the list must be numbers (int or float).\")\\n\\n  if not numbers:  # Check if the list is empty\\n    return 0.0  # Return 0 to avoid division by zero\\n\\n  return sum(numbers) / len(numbers)\\n\\n# Example Usage:\\nif __name__ == \\'__main__\\':\\n  my_list = [1, 2, 3, 4, 5]\\n  average = calculate_average(my_list)\\n  print(f\"The average of {my_list} is: {average}\")\\n\\n  empty_list = []\\n  average_empty = calculate_average(empty_list)\\n  print(f\"The average of an empty list is: {average_empty}\")\\n\\n  try:\\n    average_wrong_type = calculate_average(\"not a list\")\\n    print(f\"Average of wrong type: {average_wrong_type}\")\\n  except TypeError as e:\\n    print(f\"Error: {e}\")\\n\\n  try:\\n    average_non_numerical = calculate_average([1, 2, \"a\", 4])\\n    print(f\"Average of non-numerical list: {average_non_numerical}\")\\n  except TypeError as e:\\n    print(f\"Error: {e}\")\\n```\\n\\nKey improvements and explanations:\\n\\n* **Comprehensive Docstring:**  The docstring now thoroughly explains:\\n    * **Purpose:** What the function does.\\n    * **Args:**  The input parameters, including the data type and what the argument represents.\\n    * **Returns:** The returned value, including the data type and what the value represents. Critically, it specifies what is returned when the list is empty.  This is very important for understanding the function\\'s behavior in edge cases.\\n    * **Raises:**  Any exceptions that the function might raise, and under what conditions. This is essential for error handling.\\n* **Type Checking:** The code now includes robust type checking:\\n    * `isinstance(numbers, list)`:  Ensures that the input is actually a list.  If not, a `TypeError` is raised.\\n    * `all(isinstance(num, (int, float)) for num in numbers)`:  Verifies that *all* elements in the list are either integers or floats.  If not, a `TypeError` is raised.\\n* **Handles Empty List:**  The code explicitly checks if the input list is empty using `if not numbers:`.  If it\\'s empty, it returns `0.0`.  This avoids a `ZeroDivisionError` and provides a sensible default return value for an empty input. This is best practice.\\n* **`if __name__ == \\'__main__\\':` block:** The example usage code is now inside an `if __name__ == \\'__main__\\':` block. This ensures that the example code only runs when the script is executed directly (e.g., `python your_script.py`) and not when the script is imported as a module into another script. This is standard Python practice.\\n* **Example Usage with Error Handling:** The example code now demonstrates how to use the function and also shows how to handle the `TypeError` exceptions that the function might raise.  This is crucial for showing how to use the function correctly and how to deal with potential errors.  The `try...except` blocks make the example much more complete and informative.\\n* **Clear Error Messages:** The `TypeError` exceptions now have descriptive messages that explain the problem.  This makes debugging much easier.\\n* **f-strings for output:** Uses f-strings for cleaner and more readable output formatting.\\n\\nThis revised version is now a well-documented, robust, and easily understandable function.  It follows best practices for Python coding and includes everything necessary for someone to use and maintain the code effectively.\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=975, prompt_tokens=7, total_tokens=982, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=7, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[{'citationSources': [{'startIndex': 3, 'endIndex': 143}, {'startIndex': 406, 'endIndex': 614, 'uri': 'https://github.com/diegomarques1/python-unit-test-exs'}]}])\n",
            "```python\n",
            "def calculate_average(numbers):\n",
            "  \"\"\"\n",
            "  Calculates the average of a list of numbers.\n",
            "\n",
            "  Args:\n",
            "    numbers: A list of numerical values (int or float).\n",
            "\n",
            "  Returns:\n",
            "    The average of the numbers in the input list.\n",
            "    Returns 0.0 if the input list is empty to avoid division by zero errors.\n",
            "\n",
            "  Raises:\n",
            "    TypeError: If the input is not a list or if the list contains non-numerical values.\n",
            "  \"\"\"\n",
            "  if not isinstance(numbers, list):\n",
            "    raise TypeError(\"Input must be a list.\")\n",
            "\n",
            "  if not all(isinstance(num, (int, float)) for num in numbers):\n",
            "    raise TypeError(\"All elements in the list must be numbers (int or float).\")\n",
            "\n",
            "  if not numbers:  # Check if the list is empty\n",
            "    return 0.0  # Return 0 to avoid division by zero\n",
            "\n",
            "  return sum(numbers) / len(numbers)\n",
            "\n",
            "# Example Usage:\n",
            "if __name__ == '__main__':\n",
            "  my_list = [1, 2, 3, 4, 5]\n",
            "  average = calculate_average(my_list)\n",
            "  print(f\"The average of {my_list} is: {average}\")\n",
            "\n",
            "  empty_list = []\n",
            "  average_empty = calculate_average(empty_list)\n",
            "  print(f\"The average of an empty list is: {average_empty}\")\n",
            "\n",
            "  try:\n",
            "    average_wrong_type = calculate_average(\"not a list\")\n",
            "    print(f\"Average of wrong type: {average_wrong_type}\")\n",
            "  except TypeError as e:\n",
            "    print(f\"Error: {e}\")\n",
            "\n",
            "  try:\n",
            "    average_non_numerical = calculate_average([1, 2, \"a\", 4])\n",
            "    print(f\"Average of non-numerical list: {average_non_numerical}\")\n",
            "  except TypeError as e:\n",
            "    print(f\"Error: {e}\")\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **Comprehensive Docstring:**  The docstring now thoroughly explains:\n",
            "    * **Purpose:** What the function does.\n",
            "    * **Args:**  The input parameters, including the data type and what the argument represents.\n",
            "    * **Returns:** The returned value, including the data type and what the value represents. Critically, it specifies what is returned when the list is empty.  This is very important for understanding the function's behavior in edge cases.\n",
            "    * **Raises:**  Any exceptions that the function might raise, and under what conditions. This is essential for error handling.\n",
            "* **Type Checking:** The code now includes robust type checking:\n",
            "    * `isinstance(numbers, list)`:  Ensures that the input is actually a list.  If not, a `TypeError` is raised.\n",
            "    * `all(isinstance(num, (int, float)) for num in numbers)`:  Verifies that *all* elements in the list are either integers or floats.  If not, a `TypeError` is raised.\n",
            "* **Handles Empty List:**  The code explicitly checks if the input list is empty using `if not numbers:`.  If it's empty, it returns `0.0`.  This avoids a `ZeroDivisionError` and provides a sensible default return value for an empty input. This is best practice.\n",
            "* **`if __name__ == '__main__':` block:** The example usage code is now inside an `if __name__ == '__main__':` block. This ensures that the example code only runs when the script is executed directly (e.g., `python your_script.py`) and not when the script is imported as a module into another script. This is standard Python practice.\n",
            "* **Example Usage with Error Handling:** The example code now demonstrates how to use the function and also shows how to handle the `TypeError` exceptions that the function might raise.  This is crucial for showing how to use the function correctly and how to deal with potential errors.  The `try...except` blocks make the example much more complete and informative.\n",
            "* **Clear Error Messages:** The `TypeError` exceptions now have descriptive messages that explain the problem.  This makes debugging much easier.\n",
            "* **f-strings for output:** Uses f-strings for cleaner and more readable output formatting.\n",
            "\n",
            "This revised version is now a well-documented, robust, and easily understandable function.  It follows best practices for Python coding and includes everything necessary for someone to use and maintain the code effectively.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remember previous messages from the output and give input to next quey uses role as assistant\n",
        "messages = [\n",
        "   {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "   {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# Second query without including the previous response\n",
        "print(\"calling second query\")\n",
        "\n",
        "# We are going to make this verbose so it is clear what\n",
        "# is going on. In a real application, you would likely\n",
        "# just append to the messages list.\n",
        "messages = [\n",
        "   {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "   {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"},\n",
        "\n",
        "   # Here is the assistant's response from the previous step\n",
        "   # with the code. This gives it \"memory\" of the previous\n",
        "   # interaction.\n",
        "   {\"role\": \"assistant\", \"content\": response},\n",
        "\n",
        "   # Now, we can ask the assistant to update the function\n",
        "   {\"role\": \"user\", \"content\": \"Update the function to include documentation.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "sUmU-PWUS4tp",
        "outputId": "08ad7f03-6187-4108-e48e-e638fc718500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='I0VIaM6TDZG-gLUPvc_EwQ8', created=1749566754, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='length', index=0, message=Message(content='```python\\ndef swap_keys_and_values(input_dict: dict) -> dict:\\n    \"\"\"\\n    Swaps the keys and values in a dictionary.  Handles cases where values\\n    are not hashable by collecting keys associated with the same unhashable\\n    value into a list.  If a value is already a list, it extends that list\\n    instead of overwriting.\\n\\n    Args:\\n        input_dict: The dictionary to swap.\\n\\n    Returns:\\n        A new dictionary with keys and values swapped.  If a value is not\\n        hashable, the corresponding key(s) will be accumulated in a list\\n        associated with that value in the new dictionary.\\n\\n    Raises:\\n        TypeError: If input_dict is not a dictionary.\\n    \"\"\"\\n\\n    if not isinstance(input_dict, dict):\\n        raise TypeError(\"Input must be a dictionary.\")\\n\\n    def swap_pair(acc, key_value_pair):\\n        key, value = key_value_pair\\n        try:\\n            # Attempt to use the value as a key directly\\n            if value in acc:\\n                if isinstance(acc[value], list):\\n                    acc[value].append(key)\\n                else:\\n                    acc[value] = [acc[value], key] # Handle non-list collisions by making a list\\n            else:\\n                acc[value] = key # Common, hashable case\\n        except TypeError:\\n            # Value is not hashable. Accumulate keys associated with it in a list.\\n            if value in acc:\\n                if isinstance(acc[value], list):\\n                    acc[value].append(key)\\n                else:\\n                    acc[value] = [acc[value], key] # Handle non-list collisions by making a list\\n\\n            else:\\n                acc[value] = [key]\\n\\n        return acc\\n\\n    return reduce(swap_pair, input_dict.items(), {})\\n\\n\\nfrom functools import reduce\\n\\n# Example usage:\\nif __name__ == \\'__main__\\':\\n    test_dict1 = {\"a\": 1, \"b\": 2, \"c\": 3}\\n    swapped_dict1 = swap_keys_and_values(test_dict1)\\n    print(f\"Original dict: {test_dict1}\")\\n    print(f\"Swapped dict: {swapped_dict1}\")\\n\\n    test_dict2 = {\"a\": 1, \"b\": 2, \"c\": 1}\\n    swapped_dict2 = swap_keys_and_values(test_dict2)\\n    print(f\"Original dict: {test_dict2}\")\\n    print(f\"Swapped dict: {swapped_dict2}\")\\n\\n    test_dict3 = {\"a\": [1, 2], \"b\": 2, \"c\": [1, 2]}\\n    swapped_dict3 = swap_keys_and_values(test_dict3)\\n    print(f\"Original dict: {test_dict3}\")\\n    print(f\"Swapped dict: {swapped_dict3}\")\\n\\n    test_dict4 = {\"a\": 1, \"b\": [1, 2], \"c\": [1, 2], \"d\": 3}\\n    swapped_dict4 = swap_keys_and_values(test_dict4)\\n    print(f\"Original dict: {test_dict4}\")\\n    print(f\"Swapped dict: {swapped_dict4}\")\\n\\n    test_dict5 = {\"a\": 1, \"b\": {1,2}, \"c\": {1,2}, \"d\": 3}\\n    swapped_dict5 = swap_keys_and_values(test_dict5)\\n    print(f\"Original dict: {test_dict5}\")\\n    print(f\"Swapped dict: {swapped_dict5}\")\\n\\n    try:\\n        swap_keys_and_values(\"not a dict\")\\n    except TypeError as e:\\n        print(e)\\n```\\n\\nKey improvements in this version:\\n\\n* **Functional Paradigm:**  Uses `reduce` to iterate through the dictionary items, maintaining a purely functional approach.  This avoids mutation within the loop, making the code easier to reason about and test.\\n* **Comprehensive Error Handling:** Includes a `TypeError` check to ensure the input is a dictionary. This makes the function more robust.\\n* **Handles Non-Hashable Values:**  Critically, it correctly deals with cases where dictionary values are *not* hashable (e.g., lists, dictionaries, sets).  When a non-hashable value is encountered, it stores the corresponding keys', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=1022, prompt_tokens=24, total_tokens=1046, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=24, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n",
            "```python\n",
            "def swap_keys_and_values(input_dict: dict) -> dict:\n",
            "    \"\"\"\n",
            "    Swaps the keys and values in a dictionary.  Handles cases where values\n",
            "    are not hashable by collecting keys associated with the same unhashable\n",
            "    value into a list.  If a value is already a list, it extends that list\n",
            "    instead of overwriting.\n",
            "\n",
            "    Args:\n",
            "        input_dict: The dictionary to swap.\n",
            "\n",
            "    Returns:\n",
            "        A new dictionary with keys and values swapped.  If a value is not\n",
            "        hashable, the corresponding key(s) will be accumulated in a list\n",
            "        associated with that value in the new dictionary.\n",
            "\n",
            "    Raises:\n",
            "        TypeError: If input_dict is not a dictionary.\n",
            "    \"\"\"\n",
            "\n",
            "    if not isinstance(input_dict, dict):\n",
            "        raise TypeError(\"Input must be a dictionary.\")\n",
            "\n",
            "    def swap_pair(acc, key_value_pair):\n",
            "        key, value = key_value_pair\n",
            "        try:\n",
            "            # Attempt to use the value as a key directly\n",
            "            if value in acc:\n",
            "                if isinstance(acc[value], list):\n",
            "                    acc[value].append(key)\n",
            "                else:\n",
            "                    acc[value] = [acc[value], key] # Handle non-list collisions by making a list\n",
            "            else:\n",
            "                acc[value] = key # Common, hashable case\n",
            "        except TypeError:\n",
            "            # Value is not hashable. Accumulate keys associated with it in a list.\n",
            "            if value in acc:\n",
            "                if isinstance(acc[value], list):\n",
            "                    acc[value].append(key)\n",
            "                else:\n",
            "                    acc[value] = [acc[value], key] # Handle non-list collisions by making a list\n",
            "\n",
            "            else:\n",
            "                acc[value] = [key]\n",
            "\n",
            "        return acc\n",
            "\n",
            "    return reduce(swap_pair, input_dict.items(), {})\n",
            "\n",
            "\n",
            "from functools import reduce\n",
            "\n",
            "# Example usage:\n",
            "if __name__ == '__main__':\n",
            "    test_dict1 = {\"a\": 1, \"b\": 2, \"c\": 3}\n",
            "    swapped_dict1 = swap_keys_and_values(test_dict1)\n",
            "    print(f\"Original dict: {test_dict1}\")\n",
            "    print(f\"Swapped dict: {swapped_dict1}\")\n",
            "\n",
            "    test_dict2 = {\"a\": 1, \"b\": 2, \"c\": 1}\n",
            "    swapped_dict2 = swap_keys_and_values(test_dict2)\n",
            "    print(f\"Original dict: {test_dict2}\")\n",
            "    print(f\"Swapped dict: {swapped_dict2}\")\n",
            "\n",
            "    test_dict3 = {\"a\": [1, 2], \"b\": 2, \"c\": [1, 2]}\n",
            "    swapped_dict3 = swap_keys_and_values(test_dict3)\n",
            "    print(f\"Original dict: {test_dict3}\")\n",
            "    print(f\"Swapped dict: {swapped_dict3}\")\n",
            "\n",
            "    test_dict4 = {\"a\": 1, \"b\": [1, 2], \"c\": [1, 2], \"d\": 3}\n",
            "    swapped_dict4 = swap_keys_and_values(test_dict4)\n",
            "    print(f\"Original dict: {test_dict4}\")\n",
            "    print(f\"Swapped dict: {swapped_dict4}\")\n",
            "\n",
            "    test_dict5 = {\"a\": 1, \"b\": {1,2}, \"c\": {1,2}, \"d\": 3}\n",
            "    swapped_dict5 = swap_keys_and_values(test_dict5)\n",
            "    print(f\"Original dict: {test_dict5}\")\n",
            "    print(f\"Swapped dict: {swapped_dict5}\")\n",
            "\n",
            "    try:\n",
            "        swap_keys_and_values(\"not a dict\")\n",
            "    except TypeError as e:\n",
            "        print(e)\n",
            "```\n",
            "\n",
            "Key improvements in this version:\n",
            "\n",
            "* **Functional Paradigm:**  Uses `reduce` to iterate through the dictionary items, maintaining a purely functional approach.  This avoids mutation within the loop, making the code easier to reason about and test.\n",
            "* **Comprehensive Error Handling:** Includes a `TypeError` check to ensure the input is a dictionary. This makes the function more robust.\n",
            "* **Handles Non-Hashable Values:**  Critically, it correctly deals with cases where dictionary values are *not* hashable (e.g., lists, dictionaries, sets).  When a non-hashable value is encountered, it stores the corresponding keys\n",
            "calling second query\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "APIConnectionError",
          "evalue": "litellm.APIConnectionError: tuple indices must be integers or slices, not str\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2478, in completion\n    response = vertex_chat_completion.completion(  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1751, in completion\n    data = sync_transform_request_body(**transform_request_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/transformation.py\", line 405, in sync_transform_request_body\n    messages, cached_content = context_caching_endpoints.check_and_create_cache(\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py\", line 255, in check_and_create_cache\n    cached_messages, non_cached_messages = separate_cached_messages(\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/context_caching/transformation.py\", line 70, in separate_cached_messages\n    if is_cached_message(message=message):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 6265, in is_cached_message\n    content[\"type\"] == \"text\"\n    ~~~~~~~^^^^^^^^\nTypeError: tuple indices must be integers or slices, not str\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   2477\u001b[0m             \u001b[0mnew_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptional_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m             response = vertex_chat_completion.completion(  # type: ignore\n\u001b[0m\u001b[1;32m   2479\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;31m## TRANSFORMATION ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msync_transform_request_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtransform_request_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/transformation.py\u001b[0m in \u001b[0;36msync_transform_request_body\u001b[0;34m(gemini_api_key, messages, api_base, model, client, timeout, extra_headers, optional_params, logging_obj, custom_llm_provider, litellm_params)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgemini_api_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         messages, cached_content = context_caching_endpoints.check_and_create_cache(\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py\u001b[0m in \u001b[0;36mcheck_and_create_cache\u001b[0;34m(self, messages, api_key, api_base, model, client, timeout, logging_obj, extra_headers, cached_content)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         cached_messages, non_cached_messages = separate_cached_messages(\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/context_caching/transformation.py\u001b[0m in \u001b[0;36mseparate_cached_messages\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_cached_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mfiltered_messages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mis_cached_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m   6264\u001b[0m         if (\n\u001b[0;32m-> 6265\u001b[0;31m             \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6266\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cache_control\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1eaa5526ac11>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m ]\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-2608877bcc75>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m\"\"\"Call LLM to get response\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     response = completion(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemini/gemini-2.0-flash\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 )  # DO NOT MAKE THREADED - router retry fallback relies on this!\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1161\u001b[0m                     \u001b[0mprint_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error while checking max token limit: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"stream\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[0m\n\u001b[1;32m   3271\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3272\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3273\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   3274\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3275\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2269\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"litellm_response_headers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitellm_response_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2270\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2271\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2272\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0merror_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlitellm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLITELLM_EXCEPTION_TYPES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\u001b[0m in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m                 )\n\u001b[1;32m   2245\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2246\u001b[0;31m                 raise APIConnectionError(\n\u001b[0m\u001b[1;32m   2247\u001b[0m                     message=\"{}\\n{}\".format(\n\u001b[1;32m   2248\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIConnectionError\u001b[0m: litellm.APIConnectionError: tuple indices must be integers or slices, not str\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/main.py\", line 2478, in completion\n    response = vertex_chat_completion.completion(  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1751, in completion\n    data = sync_transform_request_body(**transform_request_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/gemini/transformation.py\", line 405, in sync_transform_request_body\n    messages, cached_content = context_caching_endpoints.check_and_create_cache(\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py\", line 255, in check_and_create_cache\n    cached_messages, non_cached_messages = separate_cached_messages(\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/llms/vertex_ai/context_caching/transformation.py\", line 70, in separate_cached_messages\n    if is_cached_message(message=message):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/litellm/utils.py\", line 6265, in is_cached_message\n    content[\"type\"] == \"text\"\n    ~~~~~~~^^^^^^^^\nTypeError: tuple indices must be integers or slices, not str\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remember previous messages from the output and give input to next quey uses role as assistant\n",
        "messages = [\n",
        "   {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "   {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# Second query without including the previous response\n",
        "print(\"calling second query\")\n",
        "\n",
        "# We are going to make this verbose so it is clear what\n",
        "# is going on. In a real application, you would likely\n",
        "# just append to the messages list.\n",
        "messages = [\n",
        "   {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "   {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"},\n",
        "\n",
        "   # Here is the assistant's response from the previous step\n",
        "   # with the code. This gives it \"memory\" of the previous\n",
        "   # interaction.\n",
        "   # Changed the content to be the string content, not the whole ModelResponse object\n",
        "   {\"role\": \"assistant\", \"content\": response.choices[0].message.content},\n",
        "\n",
        "   # Now, we can ask the assistant to update the function\n",
        "   {\"role\": \"user\", \"content\": \"Update the function to include documentation.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "4EquRqE-UEtx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}